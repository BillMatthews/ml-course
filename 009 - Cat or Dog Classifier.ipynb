{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat or Dog Classifier\n",
    "In the previous lessons we have used images that were:\n",
    "- all the same size\n",
    "- the object we are interested in were centered in the image\n",
    "- the images only contained the object of interest\n",
    "- the images were Greyscale (black and white)\n",
    "\n",
    "The real world images are __a lot__ messier than this and typically:\n",
    "- are in colour\n",
    "- are different sizes\n",
    "- the object of interest is not always centered\n",
    "- their may be other objects in the image (that we are not interested in)\n",
    "- the image may show the object of interest at different angles\n",
    "\n",
    "This is a harder challenge than either the Digits or Fashion Datasets.\n",
    "\n",
    "In this workbook we will attempt to build a Classifier that takes an image and categorises it as either containing a cat or a dog. \n",
    "\n",
    "We will use the knowledge we have gained in the previous exercise to build a CNN based network to solve classification and evaluate how well it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some packages\n",
    "We are using the Python programming language and a set of Machine Learning packages - Importing packages for use is a common task. For this workshop you don't really need to pay that much attention to this step (but you do need to execute the cell) since we are focusing on building models. However the following is a description of what this cell does that you can read if you are interested.\n",
    "\n",
    "### Description of imports (Optional)\n",
    "You don't need to worry about this code as this is not the focus on the workshop but if you are interested in what this next cell does, here is an explaination.\n",
    "\n",
    "|Statement|Meaning|\n",
    "|---|---|\n",
    "|__import tensorflow as tf__ |Tensorflow (from Google) is our main machine learning library and we performs all of the various calculations for us and so hides much of the detailed complexity in Machine Learning. This _import_ statement makes the power of TensorFlow available to us and for convience we will refer to it as __tf__ |\n",
    "|__from tensorflow import keras__ |Tensorflow is quite a low level machine learning library which, while powerful and flexible can be confusing so instead we use another higher level framework called Keras to make our machine learning models more readable and easier to build and test. This _import_ statement makes the Keras framework available to us.|\n",
    "|__import numpy as np__ |Numpy is a Python library for scientific computing and is commonly used for machine learning. This _import_ statement makes the Keras framework available to us.|\n",
    "|__import matplotlib.pyplot as plt__ |To visualise what is happening in our network we will use a set of graphs and MatPlotLib is the standard Python library for producing Graphs so we __import__ this to enable us to make pretty graphs.|\n",
    "|__%matplotlib inline__| this is a Jupyter Notebook __magic__ commmand that tells the workbook to produce any graphs as part of the workbook and not as pop-up window.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(\"TensorFlow version is \", tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "The following cell contains a set of helper functions that makes our models a little clearer. We will not be going through these functions (since they require Python knowlege) so just make sure you have run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCatsAndDogsData():\n",
    "  # Download and extract the Data Set\n",
    "  zip_file = tf.keras.utils.get_file(origin=\"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\",\n",
    "                                    fname=\"cats_and_dogs_filtered.zip\", extract=True)\n",
    "\n",
    "  # Grab the location of the unzipped data\n",
    "  base_dir, _ = os.path.splitext(zip_file)\n",
    "\n",
    "  # Define the path to the Training and Validation Datasets\n",
    "  train_dir = os.path.join(base_dir, 'train')\n",
    "  validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "  return train_dir, validation_dir\n",
    "\n",
    "def getTrainingDirs(train_dir):\n",
    "  # Directory with our training cat pictures\n",
    "  train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "  print ('Total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "\n",
    "  # Directory with our training dog pictures\n",
    "  train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "  print ('Total training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "\n",
    "  return train_cats_dir, train_dogs_dir\n",
    "\n",
    "def getValidationDirs(validation_dir):\n",
    "   # Directory with our validation cat pictures\n",
    "  validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "  print ('Total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "\n",
    "  # Directory with our validation dog pictures\n",
    "  validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "  print ('Total validation dog images:', len(os.listdir(validation_dogs_dir)))\n",
    "\n",
    "  return validation_cats_dir, validation_dogs_dir\n",
    "\n",
    "def getCatsAndDogsImageNames(cats_dir, dogs_dir):\n",
    "  train_cats_names = os.listdir(cats_dir)\n",
    "  train_dogs_names = os.listdir(dogs_dir)\n",
    "\n",
    "  return train_cats_names, train_dogs_names\n",
    "\n",
    "\n",
    "def showImageGrid(image_dir, num_rows=2, num_cols=4):  \n",
    "  image_labels = os.listdir(image_dir)\n",
    "  num_pix = num_rows * num_cols\n",
    "  # Index for iterating over images\n",
    "  pic_index = 0\n",
    "  # Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "  fig = plt.gcf()\n",
    "  fig.set_size_inches(num_cols * 4, num_rows * 4)\n",
    "\n",
    "  pic_index += num_pix\n",
    "  next_pix = [os.path.join(image_dir, fname) \n",
    "                  for fname in image_labels[pic_index-num_pix:pic_index]]\n",
    "  \n",
    "  for i, img_path in enumerate(next_pix):\n",
    "    # Set up subplot; subplot indices start at 1\n",
    "    sp = plt.subplot(num_rows, num_cols, i + 1)\n",
    "    sp.axis('Off') # Don't show axes (or gridlines)\n",
    "\n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def printLossAndAccuracy(history):\n",
    "  acc = history.history['acc']\n",
    "  val_acc = history.history['val_acc']\n",
    "\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.plot(acc, label='Training Accuracy')\n",
    "  plt.plot(val_acc, label='Validation Accuracy')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.ylim([min(plt.ylim()),1])\n",
    "  plt.title('Training and Validation Accuracy')\n",
    "\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.plot(loss, label='Training Loss')\n",
    "  plt.plot(val_loss, label='Validation Loss')\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.ylabel('Cross Entropy')\n",
    "  plt.ylim([0,max(plt.ylim())])\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.show()\n",
    "    \n",
    "def predictImageContent(model):\n",
    "  import numpy as np\n",
    "  from google.colab import files\n",
    "  from keras.preprocessing import image\n",
    "\n",
    "  uploaded = files.upload()\n",
    "\n",
    "  for fn in uploaded.keys():\n",
    "\n",
    "    # predicting images\n",
    "    path = '/content/' + fn\n",
    "    img = image.load_img(path, target_size=(image_size, image_size))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict(images, batch_size=10)\n",
    "    print(classes[0])\n",
    "    if classes[0]>0.5:\n",
    "      print(fn + \" is a dog\")\n",
    "    else:\n",
    "      print(fn + \" is a cat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "The original dataset for this comes from https://www.kaggle.com/c/dogs-vs-cats\n",
    "\n",
    "However we will use a smaller version of the dataset, this will enable us to train our model quicker rather than spending hours waiting for the training to complete.\n",
    "\n",
    "The dataset is freely available as a zip file, so we need to download the file and then unzip it to the filesystem. Each image contains either a Cat or a Dog and is stored as a file.\n",
    "The structure of the unzipped images will be:\n",
    "\n",
    "`\\train\n",
    "        \\train\n",
    "                \\cats\n",
    "                \\dogs\n",
    "        \\validation\n",
    "                \\cats\n",
    "                \\dogs`\n",
    "\n",
    "The files under the __train__ folder will be used to train the model. This is split into __cats__ and __dogs__\n",
    "\n",
    "The files under the __validation__ folder will be used to train the model. This is split into __cats__ and __dogs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir, validation_dir = getCatsAndDogsData()\n",
    "\n",
    "train_cats_dir, train_dogs_dir = getTrainingDirs(train_dir)\n",
    "validation_cats_dir, validation_dogs_dir = getValidationDirs(validation_dir)\n",
    "\n",
    "train_cats_names, train_dogs_names = getCatsAndDogsImageNames(train_cats_dir, train_dogs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's looks at some of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some images from the Training folder\n",
    "print(\"Training Cat Images\")\n",
    "showImageGrid(train_cats_dir, num_rows=2, num_cols=4)\n",
    "\n",
    "print(\"Training Dog Images\")\n",
    "showImageGrid(train_dogs_dir, num_rows=2, num_cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from this small sample of images that there is a great deal of variety in the images including:\n",
    "- different sizes\n",
    "- different backgrounds\n",
    "- other objects (such as people's hands) in the image\n",
    "- images of cats and dogs in different poses and angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to DataGenerators\n",
    "Previously, we used datasets that were loaded directly into memory as an array of training samples but in the real-world it is more likely that we will have a set of images on a file-system. \n",
    "\n",
    "Additionally, given the size of typical image files we would not really want to load all the data into memory before we use it.\n",
    "\n",
    "Luckily, Keras provides us with a setof  DataGenerator feature that allow use to effectively access training data images from folders as needed rather than reading all data in at once.\n",
    "\n",
    "In this workbook we will be using the ImageDataGenerator and full details of this feature can be found at https://keras.io/preprocessing/image/\n",
    "\n",
    "An ImageDataGenerator is a Image Pre-Processor that is designed to take images and prepare them for use in a network and it offers us a range of pre-processing options that are done in memory without affecting the source image on file. This includes:\n",
    "- Resize the images\n",
    "- Normalise the data\n",
    "- Augment the data with random crops and transofmrations of the source image\n",
    "- Batch the images ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the images\n",
    "The images in the dataset are of different sizes and use RGB (Red, Green, Blue) values between 0 and 255. As before we need to perform some pre-processing to:\n",
    "- Resize the images to the same size\n",
    "- Normalize the RGB values to the range 0 to 1\n",
    "\n",
    "Since our data is already split into training and validation sets we will create two ImageDataGenerators; one for our training data and another for our validation data.\n",
    "\n",
    "We will construct these ImageDataGenerators to:\n",
    "- Normalise the data to the range 0 to 1\n",
    "- Resize the images to 160 x 160\n",
    "    - Images smaller than 160x160 will be enlarged\n",
    "    - Images larger than 160x160 will be reduced\n",
    "    - Non-Square images will be adjusted to be square\n",
    "- Batch up our images for training\n",
    "\n",
    "The configuration of the Data Generators might seem complex but hopefully will make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation\n",
    "ImageDataGenerators also allow us to increase the variety in our images by applying random transformations to our source images. \n",
    "\n",
    "This doesn't mean we are altering our source data, instead the ImageDataGenerator randomly selects a batch of images from our file system and loads them into memory. It then randomly applies transformations to the images in memory before passing the transformed image data to the model to use for learning.\n",
    "\n",
    "Using this technique we can increase the variety of the data we have available to us. For example given an image we can generate variations such as:\n",
    "- flipping the image vertically\n",
    "- flipping the image horizontally\n",
    "- rotating the image\n",
    "- zooming into the image\n",
    "- Shifting the image left/right/up/down\n",
    "\n",
    "Why would we do this?\n",
    "\n",
    "Augmentation allows us to incease variety in our training data which in turn strenghtens our model and usually leads to better generalisation as it reduces the number of times the same image is cycled through the model. When dealing with real-world images, the more variety we have the better.\n",
    "\n",
    "The keras `ImageDataGenerator` class allows us to perform some agumentations such as:\n",
    "- __rescale__ \n",
    "- __rotation_range__ specifies the angle range (0-180) to rotate the eimages\n",
    "- __width_shift_range__ the proportion of the image size to allow the image to shift left or right (from the center)\n",
    "- __heigh_shift_range__ the proportion of the image size to allow the image to shift up or down (from the center)\n",
    "- __shear_range__ the proportion of the image to shear the image by\n",
    "- __zoom_range__ the percentage to zoom in by\n",
    "- __horizontal_flip__ boolean to allow horizontal flips\n",
    "- __fill_mode__ the strategy to use to fill any pixels lost by the augmentation (e.g(one option is 'nearest')\n",
    "\n",
    "You can see the range of options in https://keras.io/preprocessing/image/\n",
    "\n",
    "For our purposes we will add:\n",
    "- rescale\n",
    "- rotation\n",
    "- flips\n",
    "\n",
    "To our Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want all our images to be re-sized to 160 x 160 pixels\n",
    "image_size = 160\n",
    "\n",
    "# For Training we want to use batches of 32 images at a time\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Data Generator\n",
    "The Training Data Generator will read images in batches from the Training Data folder and perform the pre-processing we need (re-sizing images and normalising the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Generator with rescale, rotation, zoon and flip data augmentations\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "                rescale=1./255,\n",
    "                rotation_range=40,\n",
    "                zoom_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                fill_mode='nearest')\n",
    "\n",
    "# Load Training images from our soruce folder in batches of 32 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                train_dir,  # Source directory for the training images\n",
    "                target_size = (image_size, image_size),\n",
    "                batch_size = batch_size,\n",
    "                # We are performing a Binary Classification (Cat or Dog)\n",
    "                class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Validation Data Generator\n",
    "The Validation Data Generator is almost identical to the Training Data Generator except that we obtain the data from a different folder in the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale all images by 1./255 - this shifts our data into the range 0-1\n",
    "validation_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "# Load Training images from our soruce folder in batches of 32 using train_datagen generator\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                validation_dir, # Source directory for the validation images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                 # We are performing a Binary Classification (Cat or Dog)\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Network\n",
    "Now it's time to create our Network\n",
    "\n",
    "### Exercise\n",
    "Use your existing knowledge about CNNs and work in your groups to decide what network archiecture you will use and each train a different network to compare the results.\n",
    "\n",
    "Skeleton code has been provided for you but you will need to add additional layers to attempt to build a good classifer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "# TODO: Specify how many filters you want in the first layer\n",
    "#       Specify the kernel size for this layer - such as (3,3)\n",
    "model.add(tf.keras.layers.Conv2D(filters=None, kernel_size=(None, None), padding='same',\n",
    "                                 activation='relu', \n",
    "                                 input_shape=(image_size, image_size, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(strides=(2, 2)))\n",
    "\n",
    "# Hidden Layer \n",
    "# TODO: Specify how many filters you want in the hidden layer \n",
    "#        Specify the size of the kernel - for example (3,3)\n",
    "model.add(tf.keras.layers.Conv2D(filters=None, padding='same', kernel_size=(None, None), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "# TODO: Optionally add one or more hidden layers by copying the above 2 lines of code\n",
    "#       and altering the number of filters, kernal and pool_sizes\n",
    "\n",
    "# TODO: Optionally add a Dropout layer after some of the Conv2D layers (but before Pooling layers)\n",
    "# You can add a dropout layer using:\n",
    "#       model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "# Output Layers\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "Previously we used `model.fit(X_data, y_data)` to train our models and passed in our data as parameters. However, since we are using DataGenerators we need to use a different method that is designed to make use of DataGenerators.\n",
    "\n",
    "In Keras we can use `model.fit_generator()` to do this and supply the Training DataGenerator. Optionally we can also specify the Validation DataGenerator and other options about how we want training to proceed (such as the number of epochs, steps per epoch)\n",
    "\n",
    "Full details about the _fit_generator()_ method can be found at https://keras.io/models/model/#fit_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "# Stop early if our Validation Loss stagnates\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = steps_per_epoch,  \n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = validation_steps,\n",
    "    callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLossAndAccuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "__In Practice - Never do this__.\n",
    "\n",
    "The dataset we used did not have a seperate Testing set so will will evaluate against our validation set. In practice we should not do this as it's not a true measure of how our model generalises but it will give us some indication of accuracy and we are only learning. In our main Image Classification Project you will do this properly :-)\n",
    "\n",
    "\n",
    "Similar to our Training and Validation ImageDataGenerators we will create a Test ImageDataGenerator and use the `model.evaluate_generator()` method to perform the evaluation against image files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Generator\n",
    "# Rescale all images by 1./255 - this shifts our data into the range 0-1\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "# Load Training images from our soruce folder in batches of 32 using train_datagen generator\n",
    "test_datagen = test_datagen.flow_from_directory(\n",
    "                validation_dir, # Source directory for the validation images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                 # We are performing a Binary Classification (Cat or Dog)\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test against our Test Set\n",
    "loss, accuracy = model.evaluate_generator(test_generator)\n",
    "print(\"Test Loss {:.4f}\".format(loss))\n",
    "print(\"Test Accuracy {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In groups consider what Human Level Performance should be for this task (i.e. detecting if an image contains a cat or a dog) based on the images you have seen.\n",
    "\n",
    "Consider how accurate the model is and compare that to your expected Human Level Performance. How well did your model perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your Model\n",
    "The Accuracy gives us a view of how well the model performed against our data, we now need to consider how we might test this model to look for potential issues.\n",
    "\n",
    "The following cell will allow you to select a file of your own choosing to test our current model.\n",
    "\n",
    "### Exercise\n",
    "Think about the task we are trying to solve (detect whether a picture contains a Cat or a Dog) and in your teams consider:\n",
    "- What images might you use to test whether our classifier correctly classifies an image as a cat or dog.\n",
    "- Use the cell below to try some images out \n",
    "    - you can download images to your machine from sites such as https://pixabay.com/ and run the cell below to load and classify the image - Use Small images if you can (makes the testing a bit faster)\n",
    "    - Find 1 image of a Cat and 1 image of a Dog that you think your model should easily identify correctly.\n",
    "    - Find 1 image of a Cat and 1 image of a Dog that you think might be challenging for your model to identify correctly (e.g. a Dog that doesn't look like a dog, or a Cat that looks like a Dog).\n",
    "    - NOTE: This only works in a CoLab environment.\n",
    "- Were you able to fool the model in a way that a human would not have been fooled?\n",
    "    - If so what should we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictImageContent(cnn_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

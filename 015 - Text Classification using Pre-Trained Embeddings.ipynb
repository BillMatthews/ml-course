{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Pre-trained Embdedding Layers\n",
    "When we are training our networks we are learning our Embdeddngs from scratch each time and the quality of these embeddings is limited to our training data and the time we spend training.\n",
    "\n",
    "In recent years, there has been a move towards using pre-trained layers and we saw an example of this when we used Pre-Trained Base Model for image classification.\n",
    "\n",
    "For text we can leverage some pre-trained embedding layers; these are layers that have previously been trained on large amounts of data and for a long time.\n",
    "\n",
    "In this workbook we will use a pre-trained embdedding layer to improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some packages\n",
    "We are using the Python programming language and a set of Machine Learning packages - Importing packages for use is a common task. For this workshop you don't really need to pay that much attention to this step (but you do need to execute the cell) since we are focusing on building models. However the following is a description of what this cell does that you can read if you are interested.\n",
    "\n",
    "### Description of imports (Optional)\n",
    "You don't need to worry about this code as this is not the focus on the workshop but if you are interested in what this next cell does, here is an explaination.\n",
    "\n",
    "|Statement|Meaning|\n",
    "|---|---|\n",
    "|__import tensorflow as tf__ |Tensorflow (from Google) is our main machine learning library and we performs all of the various calculations for us and so hides much of the detailed complexity in Machine Learning. This _import_ statement makes the power of TensorFlow available to us and for convience we will refer to it as __tf__ |\n",
    "|__from tensorflow import keras__ |Tensorflow is quite a low level machine learning library which, while powerful and flexible can be confusing so instead we use another higher level framework called Keras to make our machine learning models more readable and easier to build and test. This _import_ statement makes the Keras framework available to us.|\n",
    "|__import numpy as np__ |Numpy is a Python library for scientific computing and is commonly used for machine learning. This _import_ statement makes the Keras framework available to us.|\n",
    "|__import matplotlib.pyplot as plt__ |To visualise what is happening in our network we will use a set of graphs and MatPlotLib is the standard Python library for producing Graphs so we __import__ this to enable us to make pretty graphs.|\n",
    "|__%matplotlib inline__| this is a Jupyter Notebook __magic__ commmand that tells the workbook to produce any graphs as part of the workbook and not as pop-up window.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained GloVe embeddings\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the file\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "The following cell contains a set of helper functions that makes our models a little clearer. We will not be going through these functions (since they require Python knowlege) so just make sure you have run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "def printLossAndAccuracy(history):\n",
    "  import matplotlib.pyplot as plt\n",
    "  history_dict = history.history\n",
    "\n",
    "  acc = history_dict['acc']\n",
    "  val_acc = history_dict['val_acc']\n",
    "  loss = history_dict['loss']\n",
    "  val_loss = history_dict['val_loss']\n",
    "\n",
    "  epochs = range(1, len(acc) + 1)\n",
    "\n",
    "  # \"bo\" is for \"blue dot\"\n",
    "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "  # b is for \"solid blue line\"\n",
    "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  \n",
    "  plt.show()\n",
    "\n",
    "  plt.clf()   # clear figure\n",
    "\n",
    "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def predictStarRating(review, model, word_index):\n",
    "      encoded_review = [encodeReview(review, word_index)]\n",
    "      sequence = keras.preprocessing.sequence.pad_sequences(encoded_review,\n",
    "                                                      value=word_index[\"<PAD>\"],\n",
    "                                                      padding='post',\n",
    "                                                      maxlen=MAX_SEQUENCE_LENGTH)\n",
    "      prediction = model.predict(sequence)\n",
    "      if prediction >= 0.9:\n",
    "          return \"5 Stars\"\n",
    "      if prediction >= 0.7:\n",
    "          return \"4 Stars\"\n",
    "      if prediction >= 0.5:\n",
    "          return \"3 Stars\"\n",
    "      if prediction >= 0.3:\n",
    "              return \"2 Stars\"\n",
    "      return \"1 Star\"\n",
    "      \n",
    "\n",
    "def getWordIndex(corpus):\n",
    "  word_index = corpus.get_word_index()\n",
    "\n",
    "  # The first indices are reserved\n",
    "  word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "  word_index[\"<PAD>\"] = 0\n",
    "  word_index[\"<START>\"] = 1\n",
    "  word_index[\"<UNK>\"] = 2  # unknown\n",
    "  word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "  reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])  \n",
    "\n",
    "  return word_index, reverse_word_index\n",
    "\n",
    "def decodeReview(text, reverse_word_index):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "def encodeReview(text, word_index):\n",
    "  remove_words = [\"!\", \"$\", \"%\", \"&\", \"*\", \".\", \"?\", \"<\", \">\", \",\"]  \n",
    "  for word in remove_words:\n",
    "      text = text.replace(word, \"\")\n",
    "  \n",
    "  return [word_index[token] if token in word_index else 2 for token in text.lower().split(\" \")]\n",
    "\n",
    "def load_glove_file_embeddings():\n",
    "    # Extract the Embeddings\n",
    "    glove_file = os.path.join('.', 'glove.6B.100d.txt')\n",
    "    embeddings_index = {}\n",
    "    f = open(glove_file, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "def get_embedding_layer():\n",
    "    embeddings_index = load_glove_file_embeddings()\n",
    "    \n",
    "    # prepare embedding matrix\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    # note that we set trainable = False so as to keep the embeddings fixed\n",
    "    embedding_layer = keras.layers.Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    return embedding_layer\n",
    "\n",
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))\n",
    "\n",
    "# A dictionary mapping words to an integer index\n",
    "word_index, reverse_word_index = getWordIndex(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Textual Data\n",
    "As before we will Pad our sequences to be a standard length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will pad our Training  and Testing reviews so they are all 256 words long and truncate any that are longer\n",
    "# We will add any padding to the end of the sentence.\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        truncating='post',\n",
    "                                                        maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                        truncating='post',\n",
    "                                                       maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your Model\n",
    "Before we can build our model we need to create our Embedding layer. This is not as straightforward to do as it was for our Image Classification; this will probably change in the future.\n",
    "\n",
    "To support the creation of this pre-trained layer we have created some functions in the Helper Functions section - if you want you can review these functions but it is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your Model\n",
    "## Using RNN layer\n",
    "\n",
    "## Exercise\n",
    "As before we have provided a skeleton model that consists of the Input/Embedding layer and the Output Layer. We have also provided you with a single hidden layer showing an example of a 1D Convolutional network.\n",
    "\n",
    "Review the model definition and in your teams decide on the structure of the models you want to train and evaluate. Then each of you implement one of these designs.\n",
    "\n",
    "Options for your hidden layers include:\n",
    "- Having a single Convolution layer (and Pooling) followed by one or more Dense layers\n",
    "- Having a sequence of Convolution (and Pooling) layer connected to the output layer\n",
    "- Having a sequence of Convolution (and Pooling) layer followed by one or more Dense layers\n",
    "\n",
    "At each layer you should consider and specify the number of filter/units, the kernel and pool sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
    "vocab_size = 10000\n",
    "\n",
    "embedding_layer = get_embedding_layer()\n",
    "# Classification using CNN\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'))\n",
    "model.add(embedding_layer)\n",
    "\n",
    "\n",
    "# Hidden Layers\n",
    "# TODO: Create either a CNN or LSTM based hidden layer\n",
    "#model.add(keras.layers.LSTM(units = 50))\n",
    "model.add(keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Output Layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Print the Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "We are now in a position to train our model against our data. We have set the number of epochs to 50 with the option to stop early if the validation loss stagnates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll train for some epochs\n",
    "epochs = 50\n",
    "\n",
    "# Stop early if our Validation Loss stagnates\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train our model\n",
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose=2,\n",
    "                   callbacks=[early_stop])\n",
    "print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "We will now evaluate our model against a set of previously unseen reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print (\"Test Loss:\", val_loss)\n",
    "print (\"Test Accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLossAndAccuracy(cnn_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execise\n",
    "We have a method that will predict a star rating for given review. A really positive review will be predicted as 5 Stars, whereas a really negative review will be predicted as 1 Star.\n",
    "\n",
    "Try some sample reviews by changing the value of _my_review_ and see if the predicted Star rating matches your expectations.\n",
    "\n",
    "Given this system and the specific requirement to estimate a Star rating, what are the risks you should test for?\n",
    "\n",
    "Work in your teams and:\n",
    "- List out some of the risks you would want to test for\n",
    "- Try out some of your ideas out and record any issues you find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a review out\n",
    "my_review = \"ok directing but good casting.\"\n",
    "\n",
    "print(my_review)\n",
    "print(\"\\nPredicted Star Rating (1-5 Stars) is {}\".format(predictStarRating(my_review, \n",
    "                                                         model, word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

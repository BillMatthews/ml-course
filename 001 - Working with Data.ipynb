{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 002 - Feature Engineering and Data Pre-Processing\n",
    "This workbook introduces you to the basics of Feature Engineering\n",
    "\n",
    "## What is a Feature\n",
    "With Deep Learning we provide a model with a set of _training examples_ (or instances); each example contains a set of data related to that example.\n",
    "\n",
    "For example, consider the following data that we could use to train a model:\n",
    "\n",
    "|temperature|month|day|rain|snow|wind speed|wind direction|\n",
    "|---|---|---|---|---|---|---|\n",
    "|15|September|21|0|0|5|SW|\n",
    "|9|September|22|2|0|16|SW|\n",
    "|21|September|23|4|0|5|NE|\n",
    "\n",
    "If we were building a model to preduct the temperature (the __target__) we could use the data in the other columns to make that prediction. The columns _month_, _day_, _rain_, _snow_, _wind speed_ and _wind direction_ are all __Features__\n",
    "\n",
    "Each row is a training sample and each sample has the same set of Features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Engineer Features?\n",
    "Usually when we talk about Feature Engineering we are concerned with using our existing knowedlge about the Domain to create new Features to aid learning. This knowledge can reduce the complexity of the data.\n",
    "\n",
    "For example, consider a model where we want to predict the time it will take to travel between 2 points in a city based on the time of the year, time of the day and the GPS co-ordinates for the pick-up and drop-off locations. \n",
    "\n",
    "As Domain Experts we might look at the available data and decide that to aid learning we probably need some additional Features such as:\n",
    "* the distance between the pick-up and drop-off points. We don't have the route but we can estimate this based on the distance between the two points. The road network in the city is quite dense so this is a good approximation.\n",
    "* Pick-up & Drop Off Zones since the the GPS co-ordinates are quite accurate but in reality there is little difference between a pick-ups that are close to each other (e.g. within 100m). Uncovering a releationship using the fine grained GPS co-ordinates may be problematic. So we could Zone or sectorise the map in some way and engineer a higher level feature for each trip as a zone to zone trip rather than GPS to GPS. This might be just as accurate for the job and we could then simplify our data by removing the GPS co-ordinates from the training Features.\n",
    "\n",
    "While Feature Engineering seems like a good idea, there is a risk that by doing this we loose an oppertunity to uncover some new relationships or bias the learned model to existing (possibly outdated) domain models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "Real-world data is messy and comes in a variety of forms, not all of which are useful for Machine Learning and so it's often the case that we need to perform some Data Pre-Processing before we can use the data for learning.\n",
    "\n",
    "Common Pre-Processing tasks include:\n",
    "* Getting rid of data that is not relevant to the task\n",
    "* Converting Categorical Data into a numerical representation\n",
    "* Normalising numerical data so it has similar scales and shapes\n",
    "* Dealing with missing data for features\n",
    "* Encoding non-numerical data (e.g. words) into a numerical format\n",
    "\n",
    "What ever processing we do for the training data we must remember to do the same pre-processing on data we use to test and evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "This workbook contains a set of exercises designed to show both Feature Engineering and Data Pre-Processing in action.\n",
    "\n",
    "Since, this course does not expect development skills in Python you will not be expected to write the feature engineering or data pre-processing methods yourself. However you will want to be able to recognise what needs to be done and use pre-created functions during future exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get our dataset\n",
    "We have a data set containing a range of values to experiment with Feature Engineering and Data Pre-processing.\n",
    "We are using the __Pandas__ library to perfom our data pre-processing since it contains various functions to help with data pre-processing and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://github.com/BillMatthews/ml-course/blob/master/taxi.zip?raw=true', \n",
    "                 compression='zip', parse_dates=['pickup_datetime'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing exercises\n",
    "### Exercise: Dealing with missing data\n",
    "First we want to see if there are is any missing data. \n",
    "\n",
    "We can use the _Pandas_ __isnull()__ method to determine this and group by feature using the following command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 5 training examples where the _dropoff_longitude_ or _dropoff_latitude_ contain null values. These are not going to be any use to us.\n",
    "\n",
    "We generally have 4 options:\n",
    "* Delete the rows with null data\n",
    "* Remove the feature from the dataset\n",
    "* Replace the null values with some suitable value (e.g. some median/average value)\n",
    "* Replace the null values with some predicted value (e.g. use another model to predict the likely value)\n",
    "\n",
    "Each of these are appropraite methods for dealing with null data. As a hueristic:\n",
    "- if the number of rows is small (compared to the overall data set), then deleting the values is usally appropriate.\n",
    "   - this is the most straightforward option but reduces the amount of data available for training\n",
    "- If you have a large number of missing values for a feature, consider removing the feature\n",
    "    - this is straightforward to do but if the presence of the data is important to the model then our model may not be as accurate.\n",
    "- If we can approximate or predict a value then do this\n",
    "    - depending upon the number of missing values this might bias the learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ What should we do about the the training samples with null values? Can we easily predict/estimate values for these?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTotal samples: {}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example we will drop these values - but make sure you understand why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will drop any rows with nulls\n",
    "data = data.dropna()\n",
    "\n",
    "print(data.isnull().sum())\n",
    "print(\"\\nTotal samples: {}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with invalid data\n",
    "If we summarise our data we can see that there are some issues with the data:\n",
    "\n",
    "Similar to dealing with null values, we can deal with invalid data in one of 4 ways:\n",
    "- Delete the rows with invalid data\n",
    "- Remove the feature from the dataset\n",
    "- Replace the null values with some suitable value (e.g. some median/average value)\n",
    "- Replace the null values with some predicted value (e.g. use another model to predict the likely value)\n",
    "\n",
    "__Question__: what problems do you see in the data - hint, look at the min and max values for each feature.\n",
    "\n",
    "__Question__: Of the above options, how do you think we should deal with the invalid data for each Feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with invalid fares\n",
    "The __fare_amount__ has a range between $-44.9 and $500. It is clear that any fare below 0 is invalid; in fact there is usaully an initial charge for a taxi (approx $2.5) so it's likely that any fares below that figure are invalid. \n",
    "\n",
    "We could also argue that very large fares are also _potentially_ invalid and so we may want to limit our range to fares that are below some value such as $50.00.\n",
    "\n",
    "We could replace these with the mean value (Â£11.35) but since the fare is likely to be related to the distance travelled we might skew the results if we follow a replacement strategy like this.\n",
    "\n",
    "So let's check how many fares are below $2.50 and see if we can safely drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data[data['fare_amount'] < 2.5].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are only 41 records (out of nearly 0.5 million) so let's delete these\n",
    "data = data[data['fare_amount'] >= 2.5]\n",
    "print(\"\\nTotal samples: {}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with invalid Longitude and Latitudes\n",
    "The data is from New York City which exists between approximately:\n",
    "- Longitude -74.05 and -73.75\n",
    "- Latitude 40.63 and 40.85\n",
    "\n",
    "However looking at the stats from the data we have data that is considerably outside this and actually invalid (long and lat values are in the range -90 to +90). So we can consider anything outside of these as invalid.\n",
    "\n",
    "It's unlikely that we can provide reasonable defaults or estimate these and additionally we need these to determine the distance travelled. Therefore we need to consider deleting these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of longitude for NYC\n",
    "nyc_min_longitude = -74.05\n",
    "nyc_max_longitude = -73.75\n",
    "\n",
    "# range of latitude for NYC\n",
    "nyc_min_latitude = 40.63\n",
    "nyc_max_latitude = 40.85\n",
    "\n",
    "# The processing to strip out long and lat is complex so we will create a function to do this\n",
    "def get_NYC_records_only(dataset):\n",
    "    # Create a copy and leave the original alone\n",
    "    df2 = dataset.copy(deep=True)\n",
    "\n",
    "    for long in ['pickup_longitude', 'dropoff_longitude']:\n",
    "        df2 = df2[(df2[long] > nyc_min_longitude) & (df2[long] < nyc_max_longitude)]\n",
    "\n",
    "    for lat in ['pickup_latitude', 'dropoff_latitude']:\n",
    "        df2 = df2[(df2[lat] > nyc_min_latitude) & (df2[lat] < nyc_max_latitude)]\n",
    "    \n",
    "    return df2\n",
    "\n",
    "\n",
    "# Determine how many records are outside the Long and Lat for NYC\n",
    "nyc_data = get_NYC_records_only(data)\n",
    "num_nyc_records = nyc_data.shape[0]\n",
    "all_records = data.shape[0]\n",
    "outside_nyc_records = all_records - num_nyc_records\n",
    "percentage_outside = outside_nyc_records / all_records * 100\n",
    "\n",
    "print(\"Outside NYC longitude: {}\".format(outside_nyc_records))\n",
    "print(\"Percentage outside NYC longitude: {:0f}\".format(percentage_outside))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about 3% of our records are outside the target area - we don't really have much choice but to delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_NYC_records_only(data)\n",
    "print(\"\\nTotal samples: {}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with invalid Passenger Count\n",
    "We can see that there are some records where the passenger count is 0 which seems odd.\n",
    "\n",
    "They could be trips made to deliver some package rather than to carry a passenger but that seems rare in this day due to the security concerns of doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Records with 0 passengers: {}\".format(data[data['passenger_count'] < 1.0].shape[0]))\n",
    "\n",
    "data['passenger_count'].plot.hist(bins=6)\n",
    "plt.xlabel('Num Passengers')\n",
    "plt.title('Passenger Count Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 1,700 records falling into the category we could just delete the rows but ideally we want to keep as much data as possible. We could estimate the number of passengers instead. From the statistics and the above graph we can we can see that the average/mean value is 1.6 passengers and actually by far the most common is a single passenger.\n",
    "\n",
    "So replacing any 0 passenger count values a value of 1 acceptable and the number of records shouldn't bias our model much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mode (the most frequent value)\n",
    "mode = data['passenger_count'].mode().values[0]\n",
    "# Set the passenger count to the mode where the passenger count is currently 0\n",
    "data.loc[data['passenger_count'] == 0, 'passenger_count'] = mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with un-needed features\n",
    "If you look at the data you will see that we data duplicated in two columns _key_ and _pickup_datetime_. We don't really need both so we should delete one of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the key column\n",
    "data.drop(['key'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Data Processing\n",
    "We performed some basic analysis on our data and found some anomalies and dealt with them using different means. Specifically we:\n",
    "- Deleted samples where the Fare was below the minimum\n",
    "- Deleted samples where the Longitude and Latitude of the Journey was outside NYC\n",
    "- Deleted samples where the sample included null values (in the Longitude and Latitude)\n",
    "- Updated samples where the passenger count was 0 to reflect the mode of the dataset.\n",
    "\n",
    "Our data is now in a better state; we started with 500K records and now have 483K records that are reasonably clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())\n",
    "print(\"\\nTotal samples: {}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Now that our data is in a good state we can turn our attention to enriching our dataset through feature engineering.\n",
    "\n",
    "Remember that our objective is to predict the Fare to be charged based on a taxi journey. Currently we have a date and time of journey, the number of passengers and the pick-up and drop-off locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Identifying Features\n",
    "__Question__: Think about the data we have and the task we are trying to achieve. What features do you think might be useful.\n",
    "Think about what factors you think are important and how they are reflected in our data. Are these factors readily accessible to our model?\n",
    "\n",
    "Identify a set of new features you think might be useful to aid learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Feature that might be helpful is a distance metric - the fare is likely to be highly corollated with the pick-up and drop-off locations. \n",
    "\n",
    "However we don't have any information about the route taken - cities like NYC have multiple routes that could be taken depending on traffic conditions.\n",
    "\n",
    "While we can't have an accurate distance it might be useful to have an indication of the distance; something that can be calculated based on the Longitude and Latitudes of the Pick-Up and Drop-Off points.\n",
    "\n",
    "We will use a simple measure called Euclidean Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(lat1, long1, lat2, long2):\n",
    "    return (((lat1-lat2)**2 + (long1 - long2) ** 2) ** 0.5)\n",
    "\n",
    "# Now we will create a new feature called distance that uses this method\n",
    "data['distance'] = euclidean_distance(data['pickup_latitude'], \n",
    "                        data['pickup_longitude'],\n",
    "                       data['dropoff_latitude'],\n",
    "                       data['dropoff_longitude'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: Consider the _pickup_datetime_ column how easy do you think it will be to learn patterns from this? Which parts of the date are most relevant to the learning task?\n",
    "\n",
    "For example:\n",
    "- it is more likely that the day of the month or the day of the week is more relevant to estimating the fare price?\n",
    "- How relevant is it that a journey was taken at 12:30 rather than 12:15? Is there likely to be much difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's break out the date into it's parts to see if these are useful\n",
    "data['year'] = data['pickup_datetime'].dt.year\n",
    "data['month'] = data['pickup_datetime'].dt.month\n",
    "data['day'] = data['pickup_datetime'].dt.day\n",
    "data['day_of_week'] = data['pickup_datetime'].dt.dayofweek\n",
    "data['hour'] = data['pickup_datetime'].dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['day_of_week'].plot.hist(bins=np.arange(8)-0.5, ec='black',\n",
    "                             ylim=(60000, 75000))\n",
    "plt.xlabel('Day of the Week (0=Monday)')\n",
    "plt.title('Day of Week Histogram')\n",
    "plt.show()\n",
    "\n",
    "data['hour'].plot.hist(bins=24, ec='black')\n",
    "plt.xlabel('Hour')\n",
    "plt.title('Pickup by Hour Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's likely that seperating out the date is going to help with the learning.\n",
    "\n",
    "It also means that the original _pickup_datetime_ is redundant and can be dropped. We might also consider droping the year and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['pickup_datetime', 'year', 'day'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary\n",
    "We have seen some examples of engineering new features to enrich our data. Specicially we:\n",
    "- Created a new feature (_distance_) based on a combination of existing features \n",
    "- Split an existing feature to create new features to simplify the dataset.\n",
    "- Removed some (seemingly redundant) features.\n",
    "\n",
    "There are other features that we might want to try such as:\n",
    "- Zoning the Pick-up & Drop-off locations\n",
    "- Flaging whether a journey was made on a National Holiday or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In General, Feature Engineering is an iterative process. Often we will start by cleaning up our data and training the model with few engineered features. \n",
    "\n",
    "We can use this as a baseline and see if we can engineer new features to improve the model.\n",
    "\n",
    "Over engineering features (particularly early in the process) can lead to poor and inflexible models. It is generally better to start simple and iteratively add engineered features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising Data\n",
    "We have numerical data that has very different ranges; for example compare the values for _passenger_count_ and the longitude and latitude features. In some datasets, the difference in ranges can be extreme (e.g. very small values compred to very large value).\n",
    "\n",
    "This can cause issues with machine learning, leading to poor learning (slow or poor levels of accuracy) or the model becoming more sensitive to some features.\n",
    "\n",
    "To allievate this we __Normalise__ the data; this is a mathematical operation that preserves the relationship between datapoints but brings them into similar ranges.\n",
    "\n",
    "The downside of this is that the data no longer resembles to original dataset.\n",
    "\n",
    "We would often do this as the last step in preparing data for a model (after pre-processing and feature engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the data\n",
    "# get a copy of the fare_amount (since we don't want to normalise)\n",
    "temp= data['fare_amount'].copy()\n",
    "# Normalise the data\n",
    "data = tf.keras.utils.normalize(data, axis=1)\n",
    "\n",
    "# Replace our fare_amount data\n",
    "data['fare_amount'] = temp\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IMPORTANT__: We Normalise the Features but we do not Normalise the target (the value we are predicting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

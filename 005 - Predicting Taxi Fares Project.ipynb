{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Project\n",
    "In the previous workbook you were led through the steps to design and train a model. \n",
    "\n",
    "In this workbook you will train a model to predict the Fare for a New York City Taxi. \n",
    "\n",
    "This is based on a Kaggle Challenge - https://www.kaggle.com/c/new-york-city-taxi-fare-prediction\n",
    "\n",
    "The main difference is that we are using a smaller subset of the data.\n",
    "\n",
    "This workbook is more of a Project and contains less guidance and less included code so work in your teams and try and solve the challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions \n",
    "The following cell contains a set of functions that you may (or may not) use during this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of longitude for NYC\n",
    "nyc_min_longitude = -74.05\n",
    "nyc_max_longitude = -73.75\n",
    "\n",
    "# range of latitude for NYC\n",
    "nyc_min_latitude = 40.63\n",
    "nyc_max_latitude = 40.85\n",
    "\n",
    "def get_NYC_records_only(dataset):\n",
    "    return get_records_within_long_lat(dataset, nyc_min_longitude, nyc_max_longitude,\n",
    "                                      nyc_min_latitude, nyc_max_latitude)\n",
    "    \n",
    "def get_records_within_long_lat(dataset, min_long, max_long, min_lat, max_lat):\n",
    "    # Create a copy and leave the original alone\n",
    "    df2 = dataset.copy(deep=True)\n",
    "\n",
    "    for long in ['pickup_longitude', 'dropoff_longitude']:\n",
    "        df2 = df2[(df2[long] > min_long) & (df2[long] < max_long)]\n",
    "\n",
    "    for lat in ['pickup_latitude', 'dropoff_latitude']:\n",
    "        df2 = df2[(df2[lat] > min_lat) & (df2[lat] < max_lat)]\n",
    "    \n",
    "    return df2\n",
    "\n",
    "\n",
    "def plot_lat_long(df, points='Pickup'):\n",
    "    plt.figure(figsize = (12,12)) # set figure size\n",
    "    if points == 'pickup':\n",
    "        plt.plot(list(df.pickup_longitude), list(df.pickup_latitude), '.', markersize=1)\n",
    "    else:\n",
    "        plt.plot(list(df.dropoff_longitude), list(df.dropoff_latitude), '.', markersize=1)\n",
    "    \n",
    "    plt.title(\"{} Locations in NYC Illustrated\".format(points))\n",
    "\n",
    "    plt.grid(None)\n",
    "    plt.xlabel(\"Latitude\")\n",
    "    plt.ylabel(\"Longitude\")\n",
    "    plt.show()\n",
    "    \n",
    "def plotHistory(history):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error')\n",
    "  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "           label = 'Val Error')\n",
    "  plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the Data \n",
    "We will load the data in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv('./data/taxi.csv', parse_dates=['pickup_datetime'],)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "In the workbook on Data Pre-Processing and Feature Engineering we used this data set and performed a set of pre-processing steps on the data.\n",
    "\n",
    "Some of the steps are below, but following this next step you will need to make some descisions about what pre-processing and feature engineering actions you want to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with null values\n",
    "data = data.dropna()\n",
    "\n",
    "# Update the passenger count to 1 for any set to 0\n",
    "# Get the mode (the most frequent value)\n",
    "mode = data['passenger_count'].mode().values[0]\n",
    "# Set the passenger count to the mode where the passenger count is currently 0\n",
    "data.loc[data['passenger_count'] == 0, 'passenger_count'] = mode\n",
    "\n",
    "# Drop the key\n",
    "data.drop(['key'], axis=1, inplace=True)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Dealing with invalid Fares\n",
    "As indicated before the _fare_amount_ feature has some problems. Firstly there are some negative values but secondly has some very large fares whereas most are very small.\n",
    "\n",
    "The following graph shows the distribution of fares. Examine the graph and decide which fares you want to ignore. You can then run the cell to drop any training samples where the fare is outside the bounds you specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fare_amount'].plot.hist(bins=500)\n",
    "plt.xlabel('Fare')\n",
    "plt.title('Histogram of Fares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the following value to be the minimum fare you are interested in modelling\n",
    "min_acceptable_fare = 0\n",
    "\n",
    "# TODO: Change the following value to be the maximum fare you are interesting in modelling\n",
    "# If you want to incldue all then leave this at $500\n",
    "max_acceptable_fare = 50\n",
    "\n",
    "# Delete fares that are outside your limits\n",
    "data = data[(data['fare_amount'] >= min_acceptable_fare) &\n",
    "            (data['fare_amount'] <= max_acceptable_fare)]\n",
    "\n",
    "# Describe the data\n",
    "data.describe()\n",
    "# Show the distribution\n",
    "data['fare_amount'].plot.hist(bins=500)\n",
    "plt.xlabel('Fare')\n",
    "plt.title('Histogram of Fares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Dealing with invalid Longitude and Latitudes\n",
    "Previously we noted that some of the Longitude and Latitudes are outside of the NYC area and so are probably invalid.\n",
    "\n",
    "So as an initial pre-processing step we will get rid of any records that are outside NYC.\n",
    "\n",
    "We will then plot our pick-up and drop-off points to see if we want to reduce the dataset further to focus on specific areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_NYC_records_only(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lat_long(data, points='Pickup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lat_long(data, points='Drop Off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, just by plotting the pick-up & drop-off points we can see parts of the NYC street layout. We can see that we have certain areas of concentration whereas others are quite sparse.\n",
    "\n",
    "If you want you can focus your data on a specific area - we can achieve this by specifying the min and max longitude and latitudes we want to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: If you want to reduce the dataset further then specify latitude and longitude ranges\n",
    "# and run this cell\n",
    "min_long = -74.05\n",
    "max_long = -73.75\n",
    "\n",
    "# range of latitude for NYC\n",
    "min_lat = 40.63\n",
    "max_lat = 40.85\n",
    "\n",
    "data = get_records_within_long_lat(data, min_long, max_long, min_lat, max_lat)\n",
    "\n",
    "# Plot the pickups to show the difference\n",
    "plot_lat_long(data, points='Pickup')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We will create some new features:\n",
    "- A distance feature\n",
    "- Split out the date features into seperate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Distance feature\n",
    "def euclidean_distance(lat1, long1, lat2, long2):\n",
    "    return (((lat1-lat2)**2 + (long1 - long2) ** 2) ** 0.5)\n",
    "\n",
    "# Now we will create a new feature called distance that uses this method\n",
    "data['distance'] = euclidean_distance(data['pickup_latitude'], \n",
    "                        data['pickup_longitude'],\n",
    "                       data['dropoff_latitude'],\n",
    "                       data['dropoff_longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out the date into parts\n",
    "data['year'] = data['pickup_datetime'].dt.year\n",
    "data['month'] = data['pickup_datetime'].dt.month\n",
    "data['day'] = data['pickup_datetime'].dt.day\n",
    "data['day_of_week'] = data['pickup_datetime'].dt.dayofweek\n",
    "data['hour'] = data['pickup_datetime'].dt.hour\n",
    "\n",
    "# Remove the original _pickup_datetime_ feature\n",
    "data.drop(['pickup_datetime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Remove unwanted Features\n",
    "We now want to remove any unwanted feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Review the above columns and decide which columns you want to use for training\n",
    "# If you don't want to remove any columns then don't run this cell\n",
    "features_to_remove = ['col']\n",
    "data.drop(features_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing Splits\n",
    "We are now in a position to create our Training and Test splits\n",
    "\n",
    "First we will create our Feature and Target sets (X and y). Then we will use a method from the Sklearn module to create a random 20% split for the Testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in Train and Test sets\n",
    "X = data.loc[:, data.columns != 'fare_amount']\n",
    "y = data.loc[:, 'fare_amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise our Features\n",
    "We can now normalise our Training Features to ensure they are all in a similar range. We will use a Tensorflow method to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the data\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data Shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model\n",
    "We are now in a position to build a model. We will provide you with a basic structure, but the rest is up to you.\n",
    "\n",
    "### Exercise: Design your model\n",
    "Work in your groups to decide on a set of model designs to try out. The key choices you will need to make are:\n",
    " - How many units to include in your Input layer\n",
    " - How many Hidden layers to build and how many units in each\n",
    "    - You can specify a new layer using `layers.Dense(units=32, activation='relu'))\n",
    "\n",
    "Within your group try shallow networks (e.g. 1 hidden layer) and deeper networks (such as 5 layers). Also try to use different combinations of units in layers (typically we choose values such as 8, 16, 32, 64, 128, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "# TODO: Specify how many units you want in your input layer\n",
    "model.add(layers.Dense(units=None, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "# Hidden layers\n",
    "# TODO: Add one or more hidden layers\n",
    "model.add(None)\n",
    "\n",
    "# Output Layer\n",
    "model.add(layers.Dense(1)) # No need to specify activation as we are predicting a vlaue\n",
    "\n",
    "\n",
    " # We now compile our model with Loss Function and an Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "We are now in a position to train and evaluate our model.\n",
    "\n",
    "### Exercise: Epochs and Batch Size\n",
    "In the cell below we have specified 200 epochs and a batch_size of 64 - you are free to change these if you want.\n",
    "- epochs: this is the number of iterations to train for\n",
    "- batch_size: this determines how many training samples we train with at one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=200,\n",
    "                    validation_split = 0.2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your Model\n",
    "We now want to evaluate how good our model is.\n",
    "\n",
    "First we will look at the training and validation error during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will evaluate our model against the test set and work out what the average error is.\n",
    "\n",
    "In the case of this model, the Mean Absolute Error is the average amount our prediction is out by on a given fare. So the lower the better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy against test set\n",
    "val_loss, mean_abs_error, mean_squared_error = model.evaluate(X_test, y_test)\n",
    "print (\"Validation Mean Absolute Error:\", mean_abs_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing your model\n",
    "Testing a model is different to evaluating a model - with evaluation we are measuring how good the model is against the data. With testing we are interested in evaluating risks that might affect the model.\n",
    "\n",
    "## Exercise\n",
    "Consider the purpose of our model, which is to predict the fare price for a taxi journey in New York City between two points on a given date and time. We have an average error (from our evaluation of the model against the test data) is this enough to assess the quality of the model?\n",
    "\n",
    "In this exercise, work in your teams to build an outline Test Strategy/Approach for the model. In particular consider:\n",
    "- What additional testing would you recommend? \n",
    "    - See section below on Hypothosis Testing for one possible approach\n",
    "- What Oracles could you use to check your model's predictions against?\n",
    "- How should you evaluate your predictions?\n",
    "\n",
    "### Hypothosis Testing\n",
    "One approach to consider is to think in terms of what Hypotheses you have about the problem domain. These are statements you think should be true about the model's predictions such as:\n",
    "- the fare for a journey during Peak Traffic times will be higher than the same journey during low Traffic times. \n",
    "\n",
    "Sometimes these are known as _Good-old Fashioned Common Sense_ tests - things that ought to be true and we want to test our model to ensure it is consistent with these views.\n",
    "\n",
    "If you think about the problem domain you can probably come up with a number of such hypotheses.\n",
    "\n",
    "For each Hypothosis you can then consider how you would test to disprove the Hypothosis. For example:\n",
    "- if we pick random journeys and predict the fare during high and low traffic periods, if the low traffic fares are similar or higher than that the high traffic fares then the hypothosis must be false.\n",
    "\n",
    "The trick here is to think about how to disprove your Hypothisis rather than try to prove it - the latter can lead to confirmation bais.\n",
    "\n",
    "When we are evaluating our tests we need to consider how we detect if a problem has occurred. For example, if we predict a low traffic fare as $5.00 and the high traffic fare as $5.10 is that a sufficient difference to accept as evidence for our hypothosis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Based on your test approach select some test ideas that you think the model is most likely to get wrong, construct and run some tests using the harness below. \n",
    "\n",
    "Evaluate the results and decide if there is a problem or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Test Harness\n",
    "In the harness below you specify one or more sets of values to test; then run the cell to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = [\n",
    "        \n",
    "]\n",
    "predictions = model.predict(test_values)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Observations\n",
    "You have now taken a real-world set of data, made decisions about pre-processing and feature engineering and then used that data to train a model.\n",
    "\n",
    "\n",
    "## Further Questions\n",
    "- Do you think further Feature Engineering could improve our model (i.e. generate a lower Mean Absolute Error)?\n",
    "- How did shallow networks perform against deeper networks?\n",
    "- Do you think the Model is Overfitting the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

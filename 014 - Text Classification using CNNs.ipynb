{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using CNNs\n",
    "In the previous workbook we managed to create a reasonable Sentiment Classifier. You may have found that when testing the classifier it was easy to fool the model.\n",
    "\n",
    "Part of the limitations with the previous model is that it was basing the classification on the presences of words and the relationship between words in a sequence were not really being catered for.\n",
    "\n",
    "We had a similar issue with Images and resolved this with Convolutional layers to great effect. We can use a similar approach with text. With images we used a 2 dimentional Convolution layer but for sequences of text this isn't going to work. Instead we will use 1 dimentional Convolutional layers.\n",
    "\n",
    "In this workbook we will continue with our Sentiment Classifer example but implement the model using 1-D Convolutions layers instead of Dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some packages\n",
    "We are using the Python programming language and a set of Machine Learning packages - Importing packages for use is a common task. For this workshop you don't really need to pay that much attention to this step (but you do need to execute the cell) since we are focusing on building models. However the following is a description of what this cell does that you can read if you are interested.\n",
    "\n",
    "### Description of imports (Optional)\n",
    "You don't need to worry about this code as this is not the focus on the workshop but if you are interested in what this next cell does, here is an explaination.\n",
    "\n",
    "|Statement|Meaning|\n",
    "|---|---|\n",
    "|__import tensorflow as tf__ |Tensorflow (from Google) is our main machine learning library and we performs all of the various calculations for us and so hides much of the detailed complexity in Machine Learning. This _import_ statement makes the power of TensorFlow available to us and for convience we will refer to it as __tf__ |\n",
    "|__from tensorflow import keras__ |Tensorflow is quite a low level machine learning library which, while powerful and flexible can be confusing so instead we use another higher level framework called Keras to make our machine learning models more readable and easier to build and test. This _import_ statement makes the Keras framework available to us.|\n",
    "|__import numpy as np__ |Numpy is a Python library for scientific computing and is commonly used for machine learning. This _import_ statement makes the Keras framework available to us.|\n",
    "|__import matplotlib.pyplot as plt__ |To visualise what is happening in our network we will use a set of graphs and MatPlotLib is the standard Python library for producing Graphs so we __import__ this to enable us to make pretty graphs.|\n",
    "|__%matplotlib inline__| this is a Jupyter Notebook __magic__ commmand that tells the workbook to produce any graphs as part of the workbook and not as pop-up window.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "The following cell contains a set of helper functions that makes our models a little clearer. We will not be going through these functions (since they require Python knowlege) so just make sure you have run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLossAndAccuracy(history):\n",
    "  import matplotlib.pyplot as plt\n",
    "  history_dict = history.history\n",
    "\n",
    "  acc = history_dict['acc']\n",
    "  val_acc = history_dict['val_acc']\n",
    "  loss = history_dict['loss']\n",
    "  val_loss = history_dict['val_loss']\n",
    "\n",
    "  epochs = range(1, len(acc) + 1)\n",
    "\n",
    "  # \"bo\" is for \"blue dot\"\n",
    "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "  # b is for \"solid blue line\"\n",
    "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  \n",
    "  plt.show()\n",
    "\n",
    "  plt.clf()   # clear figure\n",
    "\n",
    "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def predictStarRating(review, model, word_index):\n",
    "      encoded_review = [encodeReview(review, word_index)]\n",
    "      sequence = keras.preprocessing.sequence.pad_sequences(encoded_review,\n",
    "                                                      value=word_index[\"<PAD>\"],\n",
    "                                                      padding='post',\n",
    "                                                      maxlen=256)\n",
    "      prediction = model.predict(sequence)\n",
    "      if prediction >= 0.9:\n",
    "          return \"5 Stars\"\n",
    "      if prediction >= 0.7:\n",
    "          return \"4 Stars\"\n",
    "      if prediction >= 0.5:\n",
    "          return \"3 Stars\"\n",
    "      if prediction >= 0.3:\n",
    "              return \"2 Stars\"\n",
    "      return \"1 Star\"\n",
    "      \n",
    "\n",
    "def getWordIndex(corpus):\n",
    "  word_index = corpus.get_word_index()\n",
    "\n",
    "  # The first indices are reserved\n",
    "  word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "  word_index[\"<PAD>\"] = 0\n",
    "  word_index[\"<START>\"] = 1\n",
    "  word_index[\"<UNK>\"] = 2  # unknown\n",
    "  word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "  reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])  \n",
    "\n",
    "  return word_index, reverse_word_index\n",
    "\n",
    "def decodeReview(text, reverse_word_index):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "def encodeReview(text, word_index):\n",
    "  remove_words = [\"!\", \"$\", \"%\", \"&\", \"*\", \".\", \"?\", \"<\", \">\", \",\"]  \n",
    "  for word in remove_words:\n",
    "      text = text.replace(word, \"\")\n",
    "  \n",
    "  return [word_index[token] if token in word_index else 2 for token in text.lower().split(\" \")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 25000, labels: 25000\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))\n",
    "\n",
    "# A dictionary mapping words to an integer index\n",
    "word_index, reverse_word_index = getWordIndex(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Textual Data\n",
    "As before we will Pad our sequences to be a standard length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will limit our Reviews to the first 256 words and truncate any reviews that are longer than this\n",
    "max_words = 256\n",
    "\n",
    "# We will pad our Training  and Testing reviews so they are all 256 words long and truncate any that are longer\n",
    "# We will add any padding to the end of the sentence.\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        truncating='post',\n",
    "                                                        maxlen=max_words)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                        truncating='post',\n",
    "                                                       maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your Model\n",
    "## Using 1D Convolutions\n",
    "In the Image Classification Lessons we introduced the idea of Convolutions and Pooling (a CNN) to improve image classification. While Convolutions were originally concieved for Image Processing, we can use the same idea with Textual data (it's all numbers after all!)\n",
    "\n",
    "For images we used `Conv2D()` to define a Convolutional layer that would scan an 2-D image with a 2-D filter. Our text sentences only have one dimention (length) so we use a 1-D Convolutional layer. Keras provides this as `Conv1D()` where rather than provide a 2-D _Kernal_ we specify a 1-D _Kernal_.\n",
    "\n",
    "Aside from these small difference it's very similar to the image processing with a CNN.\n",
    "\n",
    "Let's build one and see if we can improve on our previous model.\n",
    "## Exercise\n",
    "As before we have provided a skeleton model that consists of the Input/Embedding layer and the Output Layer. We have also provided you with a single hidden layer showing an example of a 1D Convolutional network.\n",
    "\n",
    "Review the model definition and in your teams decide on the structure of the models you want to train and evaluate. Then each of you implement one of these designs.\n",
    "\n",
    "Options for your hidden layers include:\n",
    "- Having a single Convolution layer (and Pooling) followed by one or more Dense layers\n",
    "- Having a sequence of Convolution (and Pooling) layer connected to the output layer\n",
    "- Having a sequence of Convolution (and Pooling) layer followed by one or more Dense layers\n",
    "\n",
    "At each layer you should consider and specify the number of filter/units, the kernel and pool sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a7b86c7cc63f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Input Layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m model.add(keras.layers.Embedding(input_dim = vocab_size, \n\u001b[0m\u001b[0;32m      6\u001b[0m                                      output_dim=32, input_length=max_words))\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
    "vocab_size = 10000\n",
    "\n",
    "# Classification using CNN\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(keras.layers.Embedding(input_dim = vocab_size, \n",
    "                                     output_dim=32, input_length=max_words))\n",
    "\n",
    "# YOUR CHANGES START HERE\n",
    "\n",
    "# Hidden Layers - CNN\n",
    "# TODO: Decide on how many Filters you want to use for each of the Convolutional Layers\n",
    "#       If you want to try additional Convolutional `layers then copy the line below to add addiitonal layers\n",
    "model.add(keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CHANGES END HERE \n",
    "\n",
    "# Output Layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Print the Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "We are now in a position to train our model against our data. We have set the number of epochs to 50 with the option to stop early if the validation loss stagnates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll train for some epochs\n",
    "epochs = 50\n",
    "\n",
    "# Stop early if our Validation Loss stagnates\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train our model\n",
    "cnn_history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose=2,\n",
    "                   callbacks=[early_stop])\n",
    "print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "We will now evaluate our model against a set of previously unseen reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = cnn_model.evaluate(test_data, test_labels)\n",
    "\n",
    "print (\"Test Loss:\", val_loss)\n",
    "print (\"Test Accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLossAndAccuracy(cnn_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execise\n",
    "We have a method that will predict a star rating for given review. A really positive review will be predicted as 5 Stars, whereas a really negative review will be predicted as 1 Star.\n",
    "\n",
    "Try some sample reviews by changing the value of _my_review_ and see if the predicted Star rating matches your expectations.\n",
    "\n",
    "Given this system and the specific requirement to estimate a Star rating, what are the risks you should test for?\n",
    "\n",
    "Work in your teams and:\n",
    "- List out some of the risks you would want to test for\n",
    "- Try out some of your ideas out and record any issues you find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a review out\n",
    "my_review = \"ok directing but good casting.\"\n",
    "\n",
    "print(my_review)\n",
    "print(\"\\nPredicted Star Rating (1-5 Stars) is {}\".format(predictStarRating(my_review, \n",
    "                                                         model, word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Exercise 1\n",
    "When we built our model some design descisions were made for you; these included:\n",
    "- The sequence length of the inputs\n",
    "- The Vocabulary size for the Embdedding layer\n",
    "- The size of the output of the Embedding layer\n",
    "\n",
    "In the next cell you have the option to change these values and re-train the model to see if you can improve the model by tweaking these parameters.\n",
    "\n",
    "Discuss in your teams, what values you want to try then implement, train and evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long do you want the review sequences to be\n",
    "max_words = 256\n",
    "\n",
    "# This determines the top set of words we will train with\n",
    "vocab_size = 10000\n",
    "\n",
    "# This determine the sequence length produced at the end of the Embedding process\n",
    "embedding_output = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a fresh set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 25000, labels: 25000\n"
     ]
    }
   ],
   "source": [
    "# Get a fresh set of data\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))\n",
    "\n",
    "# A dictionary mapping words to an integer index\n",
    "word_index, reverse_word_index = getWordIndex(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will pad our Training  and Testing reviews so they are all 256 words long and truncate any that are longer\n",
    "# We will add any padding to the end of the sentence.\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        truncating='post',\n",
    "                                                        maxlen=max_words)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                        truncating='post',\n",
    "                                                       maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 256, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 256, 32)           12320     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 1,296,417\n",
      "Trainable params: 1,296,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Classification using CNN\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(keras.layers.Embedding(input_dim = vocab_size, \n",
    "                                     output_dim=embedding_output, input_length=max_words))\n",
    "\n",
    "# YOUR CHANGES START HERE\n",
    "\n",
    "# Hidden Layers - CNN\n",
    "# TODO: Decide on how many Filters you want to use for each of the Convolutional Layers\n",
    "#       If you want to try additional Convolutional `layers then copy the line below to add addiitonal layers\n",
    "model.add(keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CHANGES END HERE \n",
    "\n",
    "# Output Layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Print the Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0927 18:46:10.307217 13064 deprecation.py:323] From C:\\Users\\Bill\\Anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "20000/20000 - 19s - loss: 0.6858 - acc: 0.5556 - val_loss: 0.6495 - val_acc: 0.6480\n",
      "Epoch 2/50\n",
      "20000/20000 - 20s - loss: 0.4711 - acc: 0.8045 - val_loss: 0.3355 - val_acc: 0.8622\n",
      "Epoch 3/50\n",
      "20000/20000 - 20s - loss: 0.2390 - acc: 0.9060 - val_loss: 0.3061 - val_acc: 0.8780\n",
      "Epoch 4/50\n",
      "20000/20000 - 17s - loss: 0.1638 - acc: 0.9414 - val_loss: 0.3143 - val_acc: 0.8794\n",
      "Epoch 5/50\n",
      "20000/20000 - 20s - loss: 0.1205 - acc: 0.9614 - val_loss: 0.3541 - val_acc: 0.8726\n",
      "Epoch 6/50\n",
      "20000/20000 - 20s - loss: 0.0879 - acc: 0.9753 - val_loss: 0.3851 - val_acc: 0.8732\n",
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "# We'll train for some epochs\n",
    "epochs = 50\n",
    "\n",
    "# Stop early if our Validation Loss stagnates\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train our model\n",
    "cnn_history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose=2,\n",
    "                   callbacks=[early_stop])\n",
    "print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print (\"Test Loss:\", val_loss)\n",
    "print (\"Test Accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
